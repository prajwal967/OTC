{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/prajwal/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn import datasets, linear_model, preprocessing, grid_search\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.externals import joblib\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "#from keras.models import Sequential\n",
    "#from keras.regularizers import l2, activity_l2\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, log_loss, accuracy_score, \\\n",
    "mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "from functools import partial\n",
    "np.random.seed(1338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_set(metric):\n",
    "    \n",
    "    global metric_score\n",
    "    global metric_grid_search\n",
    "    metric_functions = {'roc_auc_score' : [roc_auc_score, 'roc_auc'], 'average_precision_score' : \n",
    "                        [average_precision_score, 'average_precision'], 'f1_score' : [f1_score, 'f1'],\n",
    "                        'log_loss' : [log_loss, 'log_loss'], 'accuracy_score' : [accuracy_score, 'accuracy'],\n",
    "                        'mean_absolute_error' : [mean_absolute_error,'mean_absolute_error'],\n",
    "                        'mean_squared_error':[mean_squared_error, 'mean_squared_error'],\n",
    "                        'r2_score' : [r2_score, 'r2']\n",
    "                        }\n",
    "    \n",
    "    metric_score = metric_functions[metric][0]\n",
    "    metric_grid_search = metric_functions[metric][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_import(data, label_output, split = True, stratify = True, split_size = 0.3):\n",
    "    \n",
    "    global Data\n",
    "    Data = data\n",
    "    \n",
    "    #Reading the data, into a Data Frame.\n",
    "    global target_label\n",
    "    target_label = label_output\n",
    "\n",
    "    #Selcting the columns of string data type\n",
    "    names = data.select_dtypes(include = ['object'])\n",
    "    \n",
    "    #Converting string categorical variables to integer categorical variables.\n",
    "    label_encode(names.columns.tolist())\n",
    "    \n",
    "    if(target_label in names):\n",
    "        \n",
    "        columns = names.drop([target_label],axis=1).columns.tolist()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        columns = names\n",
    "        \n",
    "    #This function intializes the dataframes that will be used later in the program\n",
    "    #data_initialize()\n",
    "    \n",
    "    #Splitting the data into to train and test sets, according to user preference\n",
    "    if(split == True):\n",
    "        \n",
    "        test_data = data_split(stratify,split_size)\n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function that encodes the string values to numerical values.\n",
    "def label_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values.\n",
    "    encoder = ce.OrdinalEncoder(cols = column_names, verbose = 1)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data For Ensembling (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The dataframes will be used in the training phase of the ensemble models\n",
    "def second_level_train_data(predict_list, cross_val_X, cross_val_Y):\n",
    "    \n",
    "    #Converting the list of predictions into a dataframe, which will be used to train the stacking model.\n",
    "    global stack_X\n",
    "    stack_X = pd.DataFrame()\n",
    "    stack_X = stack_X.append(build_data_frame(predict_list))\n",
    "    \n",
    "    #Building a list that contains all the raw features, used as cross validation data for the base models.\n",
    "    global raw_features_X\n",
    "    raw_features_X = pd.DataFrame()\n",
    "    raw_features_X = raw_features_X.append(cross_val_X,ignore_index=True)\n",
    "    \n",
    "    #The data frame will contain the predictions and raw features  of the base models, for training the blending\n",
    "    #model\n",
    "    global blend_X\n",
    "    blend_X = pd.DataFrame()\n",
    "    blend_X = pd.concat([raw_features_X, stack_X], axis = 1, ignore_index = True)\n",
    "    \n",
    "    #Storing the cross validation dataset labels in the variable stack_Y, \n",
    "    #which will be used later to train the stacking and blending models.\n",
    "    global stack_Y\n",
    "    stack_Y = cross_val_Y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data For Ensembling (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The dataframes will be used in the testing phase of the ensemble models\n",
    "def second_level_test_data(predict_list, test_X, test_Y):\n",
    "    \n",
    "    #Converting the list of predictions into a dataframe, which will be used to test the stacking model.\n",
    "    global test_stack_X\n",
    "    test_stack_X = pd.DataFrame()\n",
    "    test_stack_X = test_stack_X.append(build_data_frame(predict_list))\n",
    "    \n",
    "    #Building a list that contains all the raw features, used as test data for the base models.\n",
    "    global test_raw_features_X\n",
    "    test_raw_features_X = pd.DataFrame()\n",
    "    test_raw_features_X = test_raw_features_X.append(test_X,ignore_index=True)\n",
    "    \n",
    "    #The data frame will contain the predictions and raw features of the base models, for testing the blending\n",
    "    #model\n",
    "    global test_blend_X\n",
    "    test_blend_X = pd.DataFrame()\n",
    "    test_blend_X = pd.concat([test_raw_features_X, test_stack_X], axis = 1, ignore_index = True)\n",
    "    \n",
    "    #Storing the cross validation dataset labels in the variable stack_Y, \n",
    "    #which will be used later to test the stacking and blending models.\n",
    "    global test_stack_Y\n",
    "    test_stack_Y = test_Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing datasets\n",
    "def data_split(stratify, split_size):\n",
    "    \n",
    "    global Data\n",
    "    \n",
    "    #Stratified Split\n",
    "    if(stratify == True):\n",
    "        Data, test = train_test_split(Data, test_size = split_size, stratify = Data[target_label],random_state = 0)\n",
    "        \n",
    "    #Random Split\n",
    "    else:\n",
    "        Data, test = train_test_split(Data, test_size = split_size,random_state = 0) \n",
    "        \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used to convert the predictions of the base models (numpy array) into a DataFrame.\n",
    "def build_data_frame(data):\n",
    "    \n",
    "    data_frame = pd.DataFrame(data).T\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Decision Tree model. Performing a grid search to select the optimal parameter values\n",
    "def train_decision_tree(train_X, train_Y, parameters_decision_tree):\n",
    "    \n",
    "    decision_tree_model = DecisionTreeClassifier()      \n",
    "    model_gs = grid_search.GridSearchCV(decision_tree_model, parameters_decision_tree, scoring = metric_grid_search)\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the output on a set of data, the built model is passed as a parameter, which is used to predict\n",
    "def predict_decision_tree(data_X, data_Y, decision_tree):\n",
    "    \n",
    "    predicted_values = decision_tree.predict_proba(data_X)[:, 1]\n",
    "    metric = metric_score(data_Y, predicted_values)\n",
    "    \n",
    "    return [metric,predicted_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_set_decision_tree(criterion = ['gini'], splitter = ['best'], max_depth = [None],\\\n",
    "                                min_samples_split = [2], min_samples_leaf = [1], min_weight_fraction_leaf = [0.0],\\\n",
    "                                max_features = [None], random_state = [None], max_leaf_nodes = [None],\\\n",
    "                                class_weight = [None], presort = [False]):\n",
    "    \n",
    "    parameters_decision_tree = {}\n",
    "    parameters_decision_tree['criterion'] = criterion\n",
    "    parameters_decision_tree['splitter'] = splitter\n",
    "    parameters_decision_tree['max_depth'] = max_depth\n",
    "    parameters_decision_tree['min_samples_split'] = min_samples_split\n",
    "    parameters_decision_tree['min_samples_leaf'] = min_samples_leaf\n",
    "    parameters_decision_tree['min_weight_fraction_leaf'] = min_weight_fraction_leaf\n",
    "    parameters_decision_tree['max_features'] = max_features\n",
    "    parameters_decision_tree['random_state'] = random_state\n",
    "    parameters_decision_tree['max_leaf_nodes'] = max_leaf_nodes\n",
    "    parameters_decision_tree['class_weight'] = class_weight\n",
    "    parameters_decision_tree['presort'] = presort\n",
    "    \n",
    "    return parameters_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Random Forest model. Performing a grid search to select the optimal parameter values\n",
    "def train_random_forest(train_X, train_Y, parameters_random_forest):\n",
    "    \n",
    "    random_forest_model = RandomForestClassifier()\n",
    "    model_gs = grid_search.GridSearchCV(random_forest_model, parameters_random_forest, scoring = metric_grid_search)\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the output on a set of data, the built model is passed as a parameter, which is used to predict\n",
    "def predict_random_forest(data_X, data_Y, random_forest):\n",
    "    \n",
    "    predicted_values = random_forest.predict_proba(data_X)[:, 1]\n",
    "    metric = metric_score(data_Y, predicted_values)\n",
    "    \n",
    "    return [metric,predicted_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for random forest. To perform hyper parameter optimisation a list of multiple elements can be entered\n",
    "#and the optimal value in that list will be picked using grid search\n",
    "def parameter_set_random_forest(n_estimators = [10], criterion = ['gini'], max_depth = [None],\\\n",
    "                                min_samples_split = [2], min_samples_leaf = [1], min_weight_fraction_leaf = [0.0],\\\n",
    "                                max_features = ['auto'], max_leaf_nodes = [None], bootstrap = [True],\\\n",
    "                                oob_score = [False], random_state = [None], verbose = [0],warm_start = [False],\\\n",
    "                                class_weight = [None]):\n",
    "    \n",
    "    parameters_random_forest = {}\n",
    "    parameters_random_forest['criterion'] = criterion\n",
    "    parameters_random_forest['n_estimators'] = n_estimators\n",
    "    parameters_random_forest['max_depth'] = max_depth\n",
    "    parameters_random_forest['min_samples_split'] = min_samples_split\n",
    "    parameters_random_forest['min_samples_leaf'] = min_samples_leaf\n",
    "    parameters_random_forest['min_weight_fraction_leaf'] = min_weight_fraction_leaf\n",
    "    parameters_random_forest['max_features'] = max_features\n",
    "    parameters_random_forest['random_state'] = random_state\n",
    "    parameters_random_forest['max_leaf_nodes'] = max_leaf_nodes\n",
    "    parameters_random_forest['class_weight'] = class_weight\n",
    "    parameters_random_forest['bootstrap'] = bootstrap\n",
    "    parameters_random_forest['oob_score'] = oob_score\n",
    "    parameters_random_forest['warm_start'] = warm_start\n",
    "    \n",
    "    return parameters_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Linear Regression model. Performing a grid search to select the optimal parameter values\n",
    "def train_linear_regression(train_X, train_Y, parameters_linear_regression):\n",
    "    \n",
    "    linear_regression_model = linear_model.LinearRegression()\n",
    "    train_X=StandardScaler().fit_transform(train_X)\n",
    "    model_gs = grid_search.GridSearchCV(linear_regression_model, parameters_linear_regression,\\\n",
    "                                        scoring = metric_grid_search)\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the output on a set of data, the built model is passed as a parameter, which is used to predict\n",
    "def predict_linear_regression(data_X, data_Y, linear_regression):\n",
    "    \n",
    "    data_X = StandardScaler().fit_transform(data_X)\n",
    "    predicted_values = linear_regression.predict(data_X)\n",
    "    metric = metric_score(data_Y, predicted_values)\n",
    "    \n",
    "    return [metric,predicted_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for linear regression. To perform hyper parameter optimisation a list of multiple elements can be entered\n",
    "#and the optimal value in that list will be picked using grid search\n",
    "def parameter_set_linear_regression(fit_intercept = [True], normalize = [False], copy_X = [True]):\n",
    "    \n",
    "    parameters_linear_regression = {}\n",
    "    parameters_linear_regression['fit_intercept'] = fit_intercept\n",
    "    parameters_linear_regression['normalize'] = normalize\n",
    "    \n",
    "    return parameters_linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Logistic Regression  model. Performing a grid search to select the optimal parameter values\n",
    "def train_logistic_regression(train_X, train_Y, parameters_logistic_regression):\n",
    "\n",
    "    logistic_regression_model = linear_model.LogisticRegression()\n",
    "    #train_X=StandardScaler().fit_transform(train_X)\n",
    "    model_gs = grid_search.GridSearchCV(logistic_regression_model, parameters_logistic_regression,\\\n",
    "                                        scoring = metric_grid_search)\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the output on a set of data, the built model is passed as a parameter, which is used to predict\n",
    "def predict_logistic_regression(data_X, data_Y, logistic_regression):\n",
    "    \n",
    "    #data_X = StandardScaler().fit_transform(data_X)\n",
    "    predicted_values = logistic_regression.predict_proba(data_X)[:, 1]\n",
    "    \n",
    "    if(metric_grid_search in ['f1', 'log_loss', 'accuracy', 'mean_squared_error', 'mean_absolute_error', 'r2']):\n",
    "        \n",
    "        predictions = logistic_regression.predict(data_X)\n",
    "        metric = metric_score(data_Y, predictions)\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        metric = metric_score(data_Y, predicted_values)\n",
    "    \n",
    "    return [metric,predicted_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for logistic regression. To perform hyper parameter optimisation a list of multiple elements can be entered\n",
    "#And the optimal value in that list will be picked using grid search\n",
    "def parameter_set_logistic_regression(penalty = ['l2'], dual = [False], tol = [0.0001], C = [1.0],\\\n",
    "                                      fit_intercept = [True], intercept_scaling = [1], class_weight = [None],\\\n",
    "                                      random_state = [None], solver = ['liblinear'], max_iter = [100],\\\n",
    "                                      multi_class = ['ovr'], verbose = [0], warm_start = [False]):\n",
    "    \n",
    "    parameters_logistic_regression = {}\n",
    "    parameters_logistic_regression['penalty'] = penalty\n",
    "    parameters_logistic_regression['dual'] = dual\n",
    "    parameters_logistic_regression['tol'] = tol\n",
    "    parameters_logistic_regression['C'] = C\n",
    "    parameters_logistic_regression['fit_intercept'] = fit_intercept\n",
    "    parameters_logistic_regression['intercept_scaling'] = intercept_scaling\n",
    "    parameters_logistic_regression['class_weight'] = class_weight\n",
    "    parameters_logistic_regression['solver'] = solver\n",
    "    parameters_logistic_regression['max_iter'] = max_iter\n",
    "    parameters_logistic_regression['multi_class'] = multi_class\n",
    "    parameters_logistic_regression['warm_start'] = warm_start\n",
    "    \n",
    "    return parameters_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for logistic regression. To perform hyper parameter optimisation a list of multiple elements can be entered\n",
    "#And the optimal value in that list will be picked using grid search\n",
    "def parameter_set_stacking(penalty = ['l2'], dual = [False], tol = [0.0001], C = [1.0],\\\n",
    "                                      fit_intercept = [True], intercept_scaling = [1], class_weight = [None],\\\n",
    "                                      random_state = [None], solver = ['liblinear'], max_iter = [100],\\\n",
    "                                      multi_class = ['ovr'], verbose = [0], warm_start = [False]):\n",
    "    \n",
    "    parameters_stacking = {}\n",
    "    parameters_stacking['penalty'] = penalty\n",
    "    parameters_stacking['dual'] = dual\n",
    "    parameters_stacking['tol'] = tol\n",
    "    parameters_stacking['C'] = C\n",
    "    parameters_stacking['fit_intercept'] = fit_intercept\n",
    "    parameters_stacking['intercept_scaling'] = intercept_scaling\n",
    "    parameters_stacking['class_weight'] = class_weight\n",
    "    parameters_stacking['solver'] = solver\n",
    "    parameters_stacking['max_iter'] = max_iter\n",
    "    parameters_stacking['multi_class'] = multi_class\n",
    "    parameters_stacking['warm_start'] = warm_start\n",
    "    \n",
    "    return parameters_stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The stacked ensmeble will be trained by using one or more of the base model algorithms\n",
    "#The function of the base model algorithm that will be used to train will be passed as the\n",
    "#model_function parameter and the parameters required to train the algorithm/model will be passed as the\n",
    "#model_parameters parameter\n",
    "def train_stack(data_X, data_Y, model_function, model_parameters):\n",
    "    \n",
    "    model = model_function(data_X, data_Y, model_parameters)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the output on a set of stacked data, after the stacked model has been built by using a base model\n",
    "#algorithm, hence we need the predict funcction of that base model algorithm to get the predictions\n",
    "#The predict function of the base model is passed as the predict_function parameter and its respective model is \n",
    "#passed as the model parameter\n",
    "def predict_stack(data_X, data_Y, predict_function, model):\n",
    "    \n",
    "    metric,predicted_values = predict_function(data_X, data_Y, model)\n",
    "    return [metric,predicted_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The blending ensmeble will be trained by using one or more of the base model algorithms\n",
    "#The function of the base model algorithm that will be used to train will be passed as the\n",
    "#model_function parameter and the parameters required to train the algorithm/model will be passed as the\n",
    "#model_parameters parameter\n",
    "def train_blend(data_X, data_Y, model_function, model_parameters):\n",
    "    \n",
    "    model = model_function(blend_X, data_Y, model_parameters)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicts the output on a set of blended data, after the blending model has been built by using a base model\n",
    "#algorithm, hence we need the predict function of that base model algorithm to get the predictions\n",
    "#The predict function of the base model is passed as the predict_function parameter and its respective model is \n",
    "#passed as the model parameter\n",
    "def predict_blend(data_X, data_Y, predict_function, model):\n",
    "    \n",
    "    metric,predicted_values = predict_function(test_blend_X, data_Y, model)\n",
    "    return [metric,predicted_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constructing a list (train_model_list) that contains a tuple for each base model, the tuple contains the name of \n",
    "#the function that trains the base model, and the paramters for training the base model. \n",
    "\n",
    "#Constructing a list (predict_model_list) that contains a tuple for each base model, the tuple contains the name of \n",
    "#the function that computes the predictions for the base model.\n",
    "\n",
    "#In the list computed for stacking and blending, the tuples have an additional element which is the train_stack \n",
    "#function or the train_blend function. This is done because different set of data (predictions of base models) \n",
    "#needs to be passed to the base model algorithms. These function enable performing the above procedure\n",
    "\n",
    "#These lists are constructed in such a way to enable the ease of use of the joblib library, i.e the parallel \n",
    "#module/function\n",
    "\n",
    "def construct_model_parameter_list(model_list, parameters_list, stack = False, blend = False):\n",
    "    \n",
    "    model_functions = {#'multi_layer_perceptron' : [train_multi_layer_perceptron,predict_multi_layer_perceptron],\n",
    "                       'decision_tree' : [train_decision_tree,predict_decision_tree],\n",
    "                       'random_forest' : [train_random_forest,predict_random_forest],\n",
    "                       'linear_regression' : [train_linear_regression,predict_linear_regression],\n",
    "                       'logistic_regression' : [train_logistic_regression,predict_logistic_regression]\n",
    "                      }\n",
    "    \n",
    "    train_model_list = list()\n",
    "    predict_model_list = list()\n",
    "    model_parameter_index = 0\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        if(stack == True):\n",
    "            \n",
    "            train_model_list.append((model_functions[model][0],parameters_list[model_parameter_index]\\\n",
    "                                         ,train_stack))\n",
    "            predict_model_list.append((model_functions[model][1],predict_stack))\n",
    "            \n",
    "        elif(blend == True):\n",
    "            \n",
    "            train_model_list.append((model_functions[model][0],parameters_list[model_parameter_index]\\\n",
    "                                         ,train_blend))\n",
    "            predict_model_list.append((model_functions[model][1],predict_blend))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            train_model_list.append((model_functions[model][0],parameters_list[model_parameter_index]))\n",
    "            predict_model_list.append(model_functions[model][1])\n",
    "            \n",
    "        model_parameter_index = model_parameter_index + 1\n",
    "        \n",
    "    return [train_model_list,predict_model_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function computes a list where each element is a tuple that contains the predict function of the base model\n",
    "#along with the corresponding base model object. This is done so that the base model object can be passed to the\n",
    "#predict function as a prameter to compute the predictions when using joblib's parallel module/function. \n",
    "def construct_model_predict_function_list(model_list, models,predict_model_list):\n",
    "    \n",
    "    model_index = 0\n",
    "    model_function_list = list()\n",
    "    for model in model_list:\n",
    "        \n",
    "        model_function_list.append((predict_model_list[model_index],models[model_index]))\n",
    "        model_index = model_index + 1\n",
    "    return model_function_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function calls the respective training and predic functions of the base models.\n",
    "def train_base_models(model_list, parameters_list, save_models = False):\n",
    "    \n",
    "    #print('\\nTRAINING BASE MODELS\\n')\n",
    "    \n",
    "    #Cross Validation using Stratified K Fold\n",
    "    train, cross_val = train_test_split(Data, test_size = 0.5, stratify = Data[target_label],random_state = 0)\n",
    "    \n",
    "    #Training the base models, and calculating AUC on the cross validation data.\n",
    "    #Selecting the data (Traing Data & Cross Validation Data)\n",
    "    train_Y = train[target_label]\n",
    "    train_X = train.drop([target_label],axis=1)\n",
    " \n",
    "    cross_val_Y = cross_val[target_label]\n",
    "    cross_val_X = cross_val.drop([target_label],axis=1)\n",
    "    \n",
    "    #The list of base models the user wants to train.\n",
    "    global base_model_list\n",
    "    base_model_list = model_list\n",
    "\n",
    "    \n",
    "    #No of base models that user wants to train\n",
    "    global no_of_base_models\n",
    "    no_of_base_models = len(base_model_list)\n",
    "    \n",
    "    \n",
    "    #We get the list of base model training functions and predict functions. The elements of the two lists are  \n",
    "    #tuples that have (base model training function,model parameters), (base model predict functions) respectively\n",
    "    [train_base_model_list,predict_base_model_list] = construct_model_parameter_list(base_model_list,\\\n",
    "                                                                                     parameters_list)\n",
    "    \n",
    "\n",
    "    #Training the base models parallely, the resulting models are stored which will be used for cross validation.\n",
    "    models = (Parallel(n_jobs = -1)(delayed(function)(train_X, train_Y, model_parameter)\\\n",
    "                                                   for function, model_parameter in train_base_model_list))\n",
    "\n",
    "    #A list with elements as tuples containing (base model predict function, and its respective model object) is \n",
    "    #returned. This list is used in the next step in the predict_base_models function, the list will be used in\n",
    "    #joblibs parallel module/function to compute the predictions and metric scores of the base models\n",
    "    #Appended in the following manner so it can be used in joblib's parallel module/function\n",
    "    global base_model_predict_function_list\n",
    "    base_model_predict_function_list = construct_model_predict_function_list(base_model_list, models,\\\n",
    "                                                                        predict_base_model_list)\n",
    "    predict_list, data_X, data_Y = predict_base_models(cross_val_X, cross_val_Y,mode = 'train')\n",
    "    \n",
    "    return [predict_list, data_X, data_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction: Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_base_models(data_X, data_Y,mode):\n",
    "    \n",
    "    #print('\\nTESTING/CROSS VALIDATION BASE MODELS\\n')\n",
    "    \n",
    "    predict_list = list()\n",
    "\n",
    "    #predict_multi_layer_perceptron = list()\n",
    "    predict_decision_tree = list()\n",
    "    predict_random_forest = list()\n",
    "    predict_linear_regression = list()\n",
    "    predict_logistic_regression = list()\n",
    "    \n",
    "    metric_linear_regression = list()\n",
    "    metric_logistic_regression = list()\n",
    "    metric_decision_tree = list()\n",
    "    metric_random_forest = list()\n",
    "    metric_multi_layer_perceptron = list()\n",
    "    \n",
    "    auc_predict_index = 0\n",
    "    \n",
    "    #Initializing a list which will contain the predictions of the base models and the variables that will\n",
    "    #calculate the metric score\n",
    "    model_predict_metric = {#'multi_layer_perceptron' : [predict_multi_layer_perceptron, metric_multi_layer_perceptron],\n",
    "                       'decision_tree' : [predict_decision_tree, metric_decision_tree],\n",
    "                       'random_forest' : [predict_random_forest, metric_random_forest],\n",
    "                       'linear_regression' : [predict_linear_regression, metric_linear_regression],\n",
    "                       'logistic_regression' : [predict_logistic_regression, metric_logistic_regression]\n",
    "                      }\n",
    "    \n",
    "    #Computing the AUC and Predictions of all the base models on the cross validation data parallely.\n",
    "    auc_predict_cross_val = (Parallel(n_jobs = -1)(delayed(function)(data_X, data_Y, model)\n",
    "                                               for function, model in base_model_predict_function_list))\n",
    "    \n",
    "    #Building the list which will contain all the predictions of the base models and will also display the metric\n",
    "    #scores of the base models\n",
    "    for model in base_model_list:\n",
    "        \n",
    "        #Assigning the predictions and metrics computed for the respective base model\n",
    "        model_predict_metric[model] = auc_predict_cross_val[auc_predict_index][1],\\\n",
    "        auc_predict_cross_val[auc_predict_index][0]\n",
    "        auc_predict_index = auc_predict_index + 1\n",
    "        \n",
    "        if(model == 'multi_layer_perceptron'):\n",
    "            \n",
    "            #This is done only for multi layer perceptron because the predictions returned by the multi layer \n",
    "            #perceptron model is a list of list, the below piece of code converts this nested list into a single\n",
    "            #list\n",
    "            predict_list.append(np.asarray(sum(model_predict_metric[model][0].tolist(), [])))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #The below list will contain all the predictions of the base models.\n",
    "            predict_list.append(model_predict_metric[model][0])\n",
    "        \n",
    "        #Printing the name of the base model and its corresponding metric score\n",
    "        print_metric(model,model_predict_metric[model][1])\n",
    "    \n",
    "    return [predict_list, data_X, data_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training the second level models parallely\n",
    "def train_ensemble_models(stack_model_list = [], stack_parameters_list = [], blend_model_list = [],\\\n",
    "                              blend_parameters_list = [], perform_weighted_average = False, weights_list = None,\n",
    "                          save_models = False):\n",
    "    \n",
    "    #print('\\nTRAINING ENSEMBLE MODELS\\n')\n",
    "    \n",
    "    global no_of_ensemble_models\n",
    "    \n",
    "    #This list will contain the names of the models/algorithms that have been used as second level models\n",
    "    #This list will be used later in the testing phase for identifying which model belongs to which ensemble\n",
    "    #(stacking or blending), hence the use of dictionaries as elements of the list\n",
    "    #Analogous to the base_model_list\n",
    "    global ensmeble_model_list\n",
    "    ensmeble_model_list = list()\n",
    "    \n",
    "    train_stack_model_list = list() \n",
    "    predict_stack_model_list = list()\n",
    "    train_blend_model_list = list()\n",
    "    predict_blend_model_list = list()\n",
    "    \n",
    "    #The list will be used to train the ensemble models, while using joblib's parallel\n",
    "    train_second_level_models = list() \n",
    "    \n",
    "    #Stacking will not be done if user does not enter the list of models he wants to use for stacking\n",
    "    if(stack_model_list != []):\n",
    "        \n",
    "        #Appending a dictionary that contians key-Stacking and its values/elements are the names of the \n",
    "        #models/algorithms that are used for performing the stacking procedure, this is done so that it will be easy\n",
    "        #to identify the models belonging to the stacking ensemble\n",
    "        ensmeble_model_list.append({'Stacking' : stack_model_list})\n",
    "        \n",
    "        #We get the list of stacked model training functions and predict functions. The elements of the two   \n",
    "        #lists are tuples that have(base model training function,model parameters,train_stack function),\n",
    "        #(base model predict functions,predict_stack function) respectively\n",
    "        [train_stack_model_list,predict_stack_model_list] = construct_model_parameter_list(stack_model_list,\\\n",
    "                                                                                           stack_parameters_list,\n",
    "                                                                                           stack=True)\n",
    "        \n",
    "    #Blending will not be done if user does not enter the list of models he wants to use for blending\n",
    "    if(blend_model_list != []):\n",
    "        \n",
    "        #Appending a dictionary that contians key-Blending and its values/elements are the names of the \n",
    "        #models/algorithms that are used for performing the blending procedure, this is done so that it will be easy\n",
    "        #to identify the models belonging to the blending ensemble\n",
    "        ensmeble_model_list.append({'Blending' : blend_model_list})\n",
    "\n",
    "        #We get the list of blending model training functions and predict functions. The elements of the two   \n",
    "        #lists are tuples that have(base model training function,model parameters,train_blend function),\n",
    "        #(base model predict functions,predict_blend function) respectively\n",
    "        [train_blend_model_list,predict_blend_model_list] = construct_model_parameter_list(blend_model_list,\\\n",
    "                                                                                           blend_parameters_list,\\\n",
    "                                                                                           blend=True)\n",
    "        \n",
    "    #The new list contains either the stacked models or blending models or both or remain empty depending on what \n",
    "    #the user has decided to use\n",
    "    train_second_level_models = train_stack_model_list + train_blend_model_list\n",
    "    \n",
    "    #If the user wants to perform a weighted average, a tuple containing (hyper parmeter optimisation = True/False,\n",
    "    #the lsit of weights either deafult or entered by the user, and the function that performs the weighted average)\n",
    "    #will be created. This tuple will be appended to the list above\n",
    "    #weights_list[-1] is an element of the list that indicates wwether hyper parameter optimisation needs to be\n",
    "    #perofrmed\n",
    "    #if(perform_weighted_average == True):\n",
    "        \n",
    "        #train_weighted_average_list = (weights_list[-1], weights_list, weighted_average)\n",
    "        #train_second_level_models.append(train_weighted_average_list)\n",
    "\n",
    "        \n",
    "    no_of_ensemble_models = len(train_second_level_models)\n",
    "\n",
    "    #If weighted average is performed, the last element of models will contain the metric score and weighted average\n",
    "    #predictions, and not a model object. So we use the last element in different ways compared to the other model\n",
    "    #objects\n",
    "    \n",
    "    #Training the ensmeble models parallely \n",
    "    models = Parallel(n_jobs = -1)(delayed(function)(stack_X, stack_Y, model, model_parameter)\\\n",
    "                                        for model, model_parameter, function in train_second_level_models)\n",
    "    \n",
    "    \n",
    "    #A list with elements as tuples containing((base model predict function,predict_stack or predict_blend functions)\n",
    "    #,and its respective base model object) is returned. This list is used in the next step in the   \n",
    "    #predict_ensemble_models function, the list will be used in\n",
    "    #joblibs parallel module/function to compute the predictions and metric score of the ensemble models\n",
    "    #Appended in the following manner so it can be used in joblib's parallel module/function\n",
    "    #Analogous to base_model_predict_function_list\n",
    "    global ensmeble_model_predict_function_list\n",
    "    ensmeble_model_predict_function_list = construct_model_predict_function_list(stack_model_list + blend_model_list,\\\n",
    "                                                                                 models, predict_stack_model_list \n",
    "                                                                                 + predict_blend_model_list)\n",
    "    \n",
    "    #If weighted average is needed to be perofrmed we need to append((None(which indicates its testing phase),the\n",
    "    #weighted average function),and the weights). Appended in the following manner so it can be used in joblib's\n",
    "    #parallel module/function\n",
    "    #if(perform_weighted_average == True):\n",
    "        \n",
    "        #weight = models[-1][-1]\n",
    "        #print('Weighted Average')\n",
    "        #print('Weight',weight)\n",
    "        #print('Metric Score',models[-1][0])\n",
    "        #ensmeble_model_list.append({'Weighted Average' : [str(weight)]})\n",
    "        #ensmeble_model_predict_function_list.append(((None,weighted_average),weight))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def harmony_search(predictions, index):\n",
    "    \n",
    "    global harmony_matrix\n",
    "    prediction_list = list()\n",
    "    k = 0\n",
    "    prediction_list=predictions[:]\n",
    "    for j in range(len(harmony_matrix[index])):\n",
    "        if(harmony_matrix[index][j]==0):\n",
    "            del prediction_list[k]\n",
    "        else:\n",
    "            k = k + 1\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def harmony_search_adjust(index):\n",
    "    \n",
    "    global harmony_matrix\n",
    "    global hmcr\n",
    "    global par\n",
    "    \n",
    "    for  i in range (len(harmony_matrix[index])):\n",
    "        \n",
    "        if(random.uniform(0,1)>hmcr):\n",
    "        \n",
    "            harmony_matrix[index][i] = harmony_matrix[random.randint(0,harmony_matrix.shape[0]-1)][i]\n",
    "            if(random.uniform(0,1) > par):\n",
    "                harmony_matrix[index][i] = (harmony_matrix[index][i] + 1)%2\n",
    "                \n",
    "        else:\n",
    "        \n",
    "            harmony_matrix[index][i] = random.randint(0,1)\n",
    "            \n",
    "    if(sum(harmony_matrix[index])==0):\n",
    "        harmony_matrix = np.delete(harmony_matrix, (index), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions: Ensemble Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_ensemble_models(data_X, data_Y):\n",
    "    \n",
    "    #print('\\nTESTING ENSEMBLE MODELS\\n')\n",
    "\n",
    "    metric_linear_regression = list()\n",
    "    metric_logistic_regression = list()\n",
    "    metric_decision_tree = list()\n",
    "    metric_random_forest = list()\n",
    "    #metric_multi_layer_perceptron = list()\n",
    "    metric_weighted_average = list()\n",
    "    metric_stacking = list()\n",
    "    metric_blending = list()\n",
    "    \n",
    "    auc_predict_index = 0\n",
    "    \n",
    "    #Initializing a list which will contain the predictions of the base models and the variables that will\n",
    "    #calculate the metric score\n",
    "    model_metric = {#'multi_layer_perceptron' : [metric_multi_layer_perceptron],\n",
    "                       'decision_tree' : [metric_decision_tree],\n",
    "                       'random_forest' : [metric_random_forest],\n",
    "                       'linear_regression' : [metric_linear_regression],\n",
    "                       'logistic_regression' : [metric_logistic_regression]\n",
    "                      }\n",
    "    \n",
    "    #Computing the AUC and Predictions of all the ensmeble models on the test data parallely.\n",
    "    auc_predict_cross_val = (Parallel(n_jobs = -1)(delayed(function[1])(data_X, data_Y, function[0],model)\n",
    "                                               for function, model in ensmeble_model_predict_function_list))\n",
    "    \n",
    "    #ensemble_model_list is a list defined in the train_ensemble_models function, each element of the lsit is a\n",
    "    #dictionary, that contains the name of the ensembling technique (key) and the models assocaited with it(values)\n",
    "    \n",
    "    #So the first for loop gives the dictionary\n",
    "    for ensemble_models in ensmeble_model_list:\n",
    "    \n",
    "        #This for gives the key value pair, key being the name of the ensembling technique, value being a list\n",
    "        #of the models used for that ensemble\n",
    "        for ensemble,models in ensemble_models.items():\n",
    "            \n",
    "            #This for loop gives the iterates through the models present in the models list and asssigns \n",
    "            #the metric score and prints it\n",
    "            for model in models:\n",
    "                \n",
    "                #Assigning the predictions and metrics computed for the respective ensemble model\n",
    "                model_metric[model] = auc_predict_cross_val[auc_predict_index][0]\n",
    "                auc_predict_index = auc_predict_index + 1\n",
    "        \n",
    "                #Printing the name of the ensmeble technique and its model and its corresponding metric score\n",
    "                print_metric(ensemble + \" \" + model,model_metric[model])\n",
    "                return model_metric[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_models(test_data):\n",
    "    \n",
    "    \n",
    "    #Training the base models, and calculating AUC on the test data.\n",
    "    #Selecting the data (Test Data)\n",
    "    test_Y = test_data[target_label]\n",
    "    test_X = test_data.drop([target_label],axis=1)\n",
    "    \n",
    "    prediction_list = predict_base_models(test_X,test_Y,mode='test')\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_models_accuracy():\n",
    "    \n",
    "    accuracy = predict_ensemble_models(test_stack_X,test_stack_Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_metric(model,metric_score):\n",
    "    a = 2\n",
    "    #Printing the metric score for the corresponding model.\n",
    "    #print (model,'\\n',metric_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv('bank-additional-full.csv',delimiter=';',header=0)\n",
    "harmony_matrix = np.random.randint(2, size=(5, 5))\n",
    "number_of_iterations = 10\n",
    "hmcr = 0.7\n",
    "par = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 1 0]\n",
      " [0 1 0 1 1]\n",
      " [1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(harmony_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1]\n",
      " [1 1 0 1 1]\n",
      " [0 1 0 1 1]\n",
      " [1 1 0 1 1]]\n",
      "over\n",
      "[[1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_test = data_import(Data,label_output='y')\n",
    "metric_set('roc_auc_score')\n",
    "param_dt = parameter_set_decision_tree(max_depth = [6])\n",
    "param_rf = parameter_set_random_forest()\n",
    "param_lr = parameter_set_linear_regression()\n",
    "param_l2 = parameter_set_logistic_regression()\n",
    "param_l1 = parameter_set_logistic_regression(penalty = ['l1'])\n",
    "\n",
    "predict_list,data_X,data_Y = train_base_models(['decision_tree',\\\n",
    "                                     'random_forest','linear_regression','logistic_regression',\\\n",
    "                                     'logistic_regression'],[param_dt, param_rf,param_lr, param_l2, param_l1])\n",
    "\n",
    "for k in range(number_of_iterations):\n",
    "    worst_index = 0\n",
    "    worst_accuracy = 100\n",
    "    for i in range(harmony_matrix.shape[0]):\n",
    "\n",
    "        prediction_list = harmony_search(predict_list, i)\n",
    "        second_level_train_data(prediction_list, data_X, data_Y)\n",
    "        train_ensemble_models(['logistic_regression'],[param_l2])\n",
    "        test_predictions,test_data_X,test_data_Y = test_models(data_test)\n",
    "        test_predictions_list = harmony_search(test_predictions, i)\n",
    "        second_level_test_data(test_predictions_list, test_data_X, test_data_Y)\n",
    "        accuracy = test_models_accuracy()\n",
    "        \n",
    "        if(accuracy < worst_accuracy):\n",
    "            worst_accuracy = accuracy\n",
    "            worst_index = i\n",
    "        \n",
    "    harmony_search_adjust(worst_index)\n",
    "\n",
    "    \n",
    "print(harmony_matrix)    \n",
    "prediction_list = harmony_search(predict_list, 0)\n",
    "second_level_train_data(prediction_list, data_X, data_Y)\n",
    "train_ensemble_models(['logistic_regression'],[param_l2])\n",
    "test_predictions,test_data_X,test_data_Y = test_models(data_test)\n",
    "test_predictions_list = harmony_search(test_predictions, 0)\n",
    "second_level_test_data(test_predictions_list, test_data_X, test_data_Y)\n",
    "best_accuracy = test_models_accuracy()\n",
    "\n",
    "hmsize = [1]*(harmony_matrix.shape[0]-1)\n",
    "\n",
    "for i in hmsize:\n",
    "\n",
    "    prediction_list = harmony_search(predict_list, i)\n",
    "    second_level_train_data(prediction_list, data_X, data_Y)\n",
    "    train_ensemble_models(['logistic_regression'],[param_l2])\n",
    "    test_predictions,test_data_X,test_data_Y = test_models(data_test)\n",
    "    test_predictions_list = harmony_search(test_predictions, i)\n",
    "    second_level_test_data(test_predictions_list, test_data_X, test_data_Y)\n",
    "    accuracy = test_models_accuracy()\n",
    "\n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        harmony_matrix = np.delete(harmony_matrix, (i-1), axis=0)\n",
    "    else:\n",
    "        harmony_matrix = np.delete(harmony_matrix, (i), axis=0)\n",
    "\n",
    "print('over')\n",
    "print(harmony_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['decision_tree','random_forest','linear_regression','logistic_regression','logistic_regression']\n",
    "params = [param_dt, param_rf,param_lr, param_l2, param_l1]\n",
    "new_models = list()\n",
    "new_params = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in [1,1,1,0,0]:\n",
    "    models[j]=models[j]*i \n",
    "    if(models[j]!=''):\n",
    "        new_models.append(models[j])\n",
    "        new_params.append(params[j])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
